{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial - Remote Arrow\n",
    "\n",
    "This notebook serves as an interactive tutorial (and documentation) to **Remote Arrow**.\n",
    "More broadly it introduces you to **Apache Arrow** and **Apache Arrow Flight**.\n",
    "\n",
    "**Apache Arrow** is a language-independent columnar memory format for flat and hierarchical data. <br>\n",
    "\n",
    "**Apache Arrow Flight** is a framework for high-speed transmission of Apache Arrow Tables over a network.  <br>\n",
    "It achieves that by eliminating the overhead for (de)serialization of the data, as well as allowing parallel down and uploads of data chunks.<br>\n",
    "\n",
    "**Remote Arrow** is a **custom** framework built on top of Apache Arrow Flight and aims to solve the problem of redundancy <br>\n",
    "and performance bottlenecks when handling shared and large dataset by outsourcing them to a centralized server. <br>\n",
    "This reduces the cost of data transmission, query, computation and storage of potentially large data, which makes it highly viable for big data applications. <br>\n",
    "\n",
    "\n",
    "You can learn more by reading through the *README* and sources listed below:\n",
    "#### Sources:\n",
    "\n",
    "1. [Apache Arrow](https://arrow.apache.org/ \"Apache Arrow\")\n",
    "2. [Introducing Apache Arrow Flight](https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight/ \"Introducing Apache Arrow Flight\") \n",
    "3. [Apache Arrow Flight API](https://arrow.apache.org/docs/format/Flight.html \"Apache Arrow Flight API\") \n",
    "4. [Benchmarking Apache Arrow Flight (Tanveer Ahmad, Zaid Al Ars and H. Peter Hofstee)](https://arxiv.org/pdf/2204.03032.pdf) \n",
    "5. [Apache Arrow Zero Memory Copy](https://towardsdatascience.com/apache-arrow-read-dataframe-with-zero-memory-69634092b1a)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Arrow Table\n",
    "Let's take a look at **the Arrow Table** format by loading a CSV file:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.csv as csv\n",
    "\n",
    "table = csv.read_csv('hawaii_covid.csv')\n",
    "print(table.schema)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Arrow Tables support a variety of common functions for handling tabular data, such as **join**, **sort_by**, **group_by**, **drop**, **select** or **take**:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "table.select([5]) # Get column at index 5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "table.num_rows"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you're familiar with **pandas** and/or **SQL** it may come to you as a relieve that **Arrow Table** provides similar methods and APIs. <br>\n",
    "When in doubt, you can always convert Arrow Tables into pandas dataframes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "table.take([0, 1, 2]).to_pandas() # Get row 0, 1, and 2 and conver to pandas df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **So why all the trouble with Arrow?**\n",
    "On one hand Arrow stores the data in **columnar** format which makes a lot of computational routines substantially faster,<br>\n",
    "especially when iterating through large chunks of data. That is, because a columns are stored in contiguous memory, <br>\n",
    "which is exceedingly easier to read in chunks compared to row-based memory layouts, as illustrated here [4]: <br>\n",
    "\n",
    "![image](https://arrow.apache.org/img/simd.png)\n",
    "\n",
    "Secondly, it provides a language-agnostic way to pass tabular data from one application to another. <br>\n",
    "Last but not least, it also allows for a technique called zero-copy reads. <br>\n",
    "Instead of copying the data and passing it on, we can simply point to the location in memory without ever reading it [5]. <br>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So far, we have uploaded and modified our dataset only locally. <br> \n",
    "Let's see how we can make them accessible to other clients."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Starting the server\n",
    "Remote Arrow uses one server to manage datasets and exposes a fixed set of commands (upload, download, query) to all clients in the network. <br>\n",
    "This set of commands can, however, be extended and adapted depending on your specific needs. <br>\n",
    "\n",
    "As mentioned before, Remote Arrow uses the **Apache Arrow Flight API** which specifies a list of services that need to be supported (e.g. **doGet**, **doPut**, **doAction**). <br>\n",
    "This is reminiscent of the **REST** architecture style (**PUT**, **POST**, **DELETE**, **GET**). <br> \n",
    "The bottomline is that both simply define a uniform interface. We are however free in implementing custom **actions and features**.\n",
    "\n",
    "In contrast to **REST** which uses **HTTP** for communication, **Arrow Flight** is built on top of gRPC and is also language agnostic. <br>\n",
    "This means, you can develop clients in any language as long as it follows/supports the **Arrow Flight API** [4]. <br>\n",
    "\n",
    "In this tutorial, we are going to use Python to demonstrate client and server interactions. <br>\n",
    "First, let's start the server."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "SERVER_IP = \"localhost\"\n",
    "SERVER_PORT = \"5005\"\n",
    "\n",
    "server_process = subprocess.Popen([\"python3\",\"server.py\",\"--host\", SERVER_IP,\"--port\", SERVER_PORT])\n",
    "pid = server_process.pid\n",
    "time.sleep(2) # Wait for server to boot before running other cells"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The server is now running in the background as long as the notebook is active, i.e. its kernel is running. <br>\n",
    "Of course, you can also run it in a separate terminal if you want to read the console output:\n",
    "\n",
    "`python3 server --host <HOST> --port <PORT>`\n",
    "\n",
    "For now, let's connect a client to the server and see how we can upload a local file:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from RemoteArrow import RemoteDataset\n",
    "client_A = RemoteDataset(\"hawaii_covid.csv\", hostname=SERVER_IP+\":\"+SERVER_PORT)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we have instantiated a **RemoteDataset** object with path to the local CSV file and the server's address. <br>\n",
    "Under the hood, our object (**client_A**) converted that file into an **Arrow Table** (just as above) and sent it to the server via **doPut**.\n",
    "\n",
    "It does so without having to serialize or convert the data again. The server simply receives it in chunks and reconstruct the **Arrow Table** from it.\n",
    "\n",
    "The dataset now resides on the server and is available to all clients as a **Flight** with **ID 0**. <br>\n",
    "\n",
    "You will also find that the server has stored our file in the ***/datasets*** subdirectory in **.parquet** format. <br>\n",
    "The **.parquet** format allows for highly memory-efficient compression, when writing or reading Arrow Tables. <br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client_A.list_flights()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A **Flight** is nothing but an access point that corresponds to one dataset or data chunk. <br>\n",
    "A client may ask the server for all available Flights that it can access, i.e. download or query against. <br>\n",
    "\n",
    "Technically, a data chunk is also a \"dataset\" but for the sake of this tutorial we refer to original data (e.g. a CSV file) as **datasets** <br>\n",
    "and any temporarily generated data as **data chunks**, for example the results of a query. <br>\n",
    "In any event, both are internally handled as **Arrow Tables** and exposed as Flights.<br>\n",
    "\n",
    "Furthermore, instantiating **client_A** as a **RemoteDataset** also overrides the **Arrow Table methods** with corresponding RPC calls to our server. <br>\n",
    "In other words, calling methods like **take** or **select**, as we did in the previous section above, <br>\n",
    "will not locally compute the query, but command the server to do the query for us, and return the result."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "query_result = client_A.take([0,1,2]) # Will be executed on the server\n",
    "query_result.to_pandas()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Query results are made available as new Flights\n",
    "client_A.list_flights()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's pretend to be another client (**client_B**) and connect to one of the flights. <br>\n",
    "We can **inspect()** a Flight to get some metadata first (e.g. *schema*, *num of rows*)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client_B = RemoteDataset()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client_B.inspect(0) # Inspect Flight 0\n",
    "client_B.connect(0) # Connect to Flight 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client_B.take([0,1,2,3,4,5]).to_pandas() # Call take() on Flight 0 and convert result to pandas"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that, **client_B** can now query against the original dataset (**hawaii_covid**) without ever downloading it. <br>\n",
    "This becomes incredibly efficient, when you want to query against a dataset that is extremly large, <br> or you simply don't want to download it in it's entirety.\n",
    "\n",
    "One key aspect to understand, however, is that **only the first method call** that follows the RemoteDataset object (**client_B**), <br>\n",
    "invokes a remote procedure call. In the case above that means, only **client_B.take([0,1,2,3,4,5])** is being sent and processed by the server.<br>\n",
    "The result is being returned as an Arrow Table object and **locally** converted to a pandas dataframe. <br>\n",
    "\n",
    "\n",
    "If we want to download the entire dataset or chunk of the Flight that our client is connected to, we can use **fetch()**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds = client_B.fetch() # Download the entire dataset of a Flight and store the result in ds\n",
    "\n",
    "# Convert it to pandas dataframe and continue to process it locally\n",
    "df = ds.to_pandas()\n",
    "print(\"Original num of rows: \", len(df))\n",
    "\n",
    "dropped = df.dropna()\n",
    "print(\"Num of rows without nulls: \", len(dropped))\n",
    "\n",
    "dropped[\"case_month\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, there is a lot of flexibility in switching between local and shared datasets.<br>\n",
    "Making intermediate query results publicly available also allows for tremendous synergy between clients. <br>\n",
    "\n",
    "One client may upload a huge dataset, another can compute a bunch of queries against it and the third client has access to all of the results <br>\n",
    "and can cherry pick only the ones relevant to its use-case. In any case, we are saving time and space.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Actions\n",
    "\n",
    "**Apache Arrow Flight** defines another interface to interact with Flights, so-called **Actions**. <br>\n",
    "Actions are application-specific. Our server provides Actions for **saving, deleting, and clearing Flights** as well as **shutting down the server**. <br>\n",
    "\n",
    "Any client can send actions as well as accessing the flights as shown above. However here we are using a new client **(client_C)**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client_C = RemoteDataset()\n",
    "client_C.list_actions()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client_C.action(type=\"save\", body=\"1 my_new_dataset\") # Request server to save Flight 1 as \"my_new_dataset.parquet\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client_C.action(\"clear\") # Clear all flights (not affecting .parquet files)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client_C.list_flights()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "All Flights have now been cleared and the list of available Flights is empty. <br>\n",
    "However, the files that have been saved should still be stored on our server <br>.\n",
    "\n",
    "Let's see what happens if our server crashes or reboots gracefully."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client_C.action(\"shutdown\")\n",
    "time.sleep(5) # Wait for server to properly shutdown before running other cells"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Restart the server\n",
    "server_process = subprocess.Popen([\"python3\",\"server.py\",\"--host\", SERVER_IP,\"--port\", SERVER_PORT])\n",
    "pid = server_process.pid\n",
    "\n",
    "time.sleep(2) # Wait for server to boot\n",
    "client_D = RemoteDataset()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should see that the server recovered the saved datasets. <br>\n",
    "As a final step in this tutorial, we can delete them via the \"delete\" actions. The corresponding .parquet files will also be wiped from the server."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client_D.action(\"delete\", \"0\")\n",
    "client_D.action(\"delete\", \"1\")\n",
    "\n",
    "client_D.list_flights()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that, the IDs did not disappear and were not rearranged, inorder to maintain consistency throughout the runtime of the server. <br>\n",
    "This is necessary when deleting specific Flights one by one, whereas \"clearing\" all Flights basically removes the necessity to maintain that order."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "In this tutorial, we have examined how **Remote Arrow** can help to outsource the burden of computation, storage and shared access of data on a single server. <br>\n",
    "Though still in its prototype phase, it can offer tremendous flexibility for managing shared tabular data.<br>\n",
    "This effect becomes more evident when datasets are used in a network of heterogenous applications that work in different languages, <br>\n",
    "even more so when the shared data is extremely large and/or the clients low in computational prowess.\n",
    "\n",
    "Another aspect, we have not discussed in-depth yet is the remarkable boost in transmission speed, that has been benchmarked by *Ahmad et al.* [4]. <br>\n",
    "Their publication cites that *\"Flight is able to achieve up to **6000 MB/s and 4800 MB/s throughput**\"* <br>\n",
    "and companies like **Dremio** that use Arrow Flight **perform 20x and 30x better** compared to traditional connection methods. <br>\n",
    "That means Arrow Flight becomes an even better choice for applications that require long-haul communication over the internet. <br>\n",
    "\n",
    "Regardless of the use-case, hopefully you have learned that **Arrow** especially in combination with **Arrow Flight** provides a highly performant framework for working with big data.\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.12 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}