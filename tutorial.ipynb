{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial - Remote Arrow\n",
    "\n",
    "This notebook serves as an interactive tutorial (and documentation) to **Remote Arrow**.\n",
    "More broadly it introduces you to **Apache Arrow** and **Apache Arrow Flight**.\n",
    "\n",
    "**Apache Arrow** is a language-independent columnar memory format for flat and hierarchical data. <br>\n",
    "It became increasingly popular for big data applications because of its high performance and efficacy in handling tabular data, <br>\n",
    "as well as its rich eco-system of 3rd-party libraries. <br>\n",
    "\n",
    "**Apache Arrow Flight** is a framework for high-speed transmission of Apache Arrow Tables.  <br>\n",
    "It achieves that by eliminating the overhead for (de)serialization of the data and allowing parallel data streaming. <br>\n",
    "Arrow Flight defines a set of protocols and interfaces that enables a language and platform agnostic approach to sharing data. <br>\n",
    "\n",
    "**Remote Arrow** is a **custom** framework built on top of Apache Arrow Flight and aims to solve the problem of redundancy <br>\n",
    "and performance bottlenecks when common or shared datasets have to be accessed by multiple clients. <br>\n",
    "It does so by outsourcing the ownership and computational burden to a centralized server and allowing clients to access the data through remote queries. <br>\n",
    "Each query result requested by a client is subsequently made available to all other clients. This is also true for new datasets uploaded by a client. <br>\n",
    "\n",
    "This eliminates the need for data transfers between clients, redudant computation of complex queries, or unnecessary storage of potentially large data. <br>\n",
    "\n",
    "\n",
    "You can learn more by reading through the *README* and sources listed below:\n",
    "#### Sources:\n",
    "\n",
    "1. [Apache Arrow](https://arrow.apache.org/ \"Apache Arrow\")\n",
    "2. [Introducing Apache Arrow Flight](https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight/ \"Introducing Apache Arrow Flight\") \n",
    "3. [Apache Arrow Flight API](https://arrow.apache.org/docs/format/Flight.html \"Apache Arrow Flight API\") \n",
    "4. [Benchmarking Apache Arrow Flight (Tanveer Ahmad, Zaid Al Ars and H. Peter Hofstee)](https://arxiv.org/pdf/2204.03032.pdf) \n",
    "5. [Apache Arrow Zero Memory Copy](https://towardsdatascience.com/apache-arrow-read-dataframe-with-zero-memory-69634092b1a)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Run this cell only if you're not using the Docker container: hoangln1/remote-arrow:tutorial\n",
    "!pip3 install pandas\n",
    "!pip3 install pyarrow==9.0.0\n",
    "\n",
    "# Download csv file\n",
    "!pip3 install gdown\n",
    "!gdown --id \"1Ko0pE194OMvFH0Vp_AJIBXvezF6B9NKY\"\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/site-packages (from pandas) (1.22.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pyarrow==9.0.0 in /usr/local/lib/python3.9/site-packages (9.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/site-packages (from pyarrow==9.0.0) (1.22.3)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting gdown\n",
      "  Downloading gdown-4.5.4-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/site-packages (from gdown) (4.11.1)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.9/site-packages (from gdown) (2.27.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.9/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/site-packages (from gdown) (3.8.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from gdown) (4.64.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/site-packages (from requests[socks]->gdown) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests[socks]->gdown) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests[socks]->gdown) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests[socks]->gdown) (2021.10.8)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: PySocks, gdown\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed PySocks-1.7.1 gdown-4.5.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
      "/usr/local/lib/python3.9/site-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Ko0pE194OMvFH0Vp_AJIBXvezF6B9NKY\n",
      "To: /Users/longnguyen/Desktop/Code/ICS691/remote_pyarrow_beta_devs/hawaii_covid.csv\n",
      "100%|██████████████████████████████████████| 28.6M/28.6M [00:02<00:00, 10.3MB/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Arrow Table\n",
    "Let's take a look at **the Arrow Table** format by loading a CSV file:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv as csv\n",
    "\n",
    "table = csv.read_csv('hawaii_covid.csv')\n",
    "print(table.schema)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "case_month: string\n",
      "res_state: string\n",
      "state_fips_code: int64\n",
      "res_county: string\n",
      "county_fips_code: int64\n",
      "age_group: string\n",
      "sex: string\n",
      "race: string\n",
      "ethnicity: string\n",
      "case_positive_specimen_interval: int64\n",
      "case_onset_interval: int64\n",
      "process: string\n",
      "exposure_yn: string\n",
      "current_status: string\n",
      "symptom_status: string\n",
      "hosp_yn: string\n",
      "icu_yn: string\n",
      "death_yn: string\n",
      "underlying_conditions_yn: string\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Arrow Tables support a variety of common functions for handling tabular data, such as **join**, **sort_by**, **group_by**, **drop**, **select** or **take**:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "table.select([5]) # Get column at index 5"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "age_group: string\n",
       "----\n",
       "age_group: [[\"18 to 49 years\",\"0 - 17 years\",\"18 to 49 years\",\"18 to 49 years\",\"18 to 49 years\",...,\"50 to 64 years\",\"18 to 49 years\",\"65+ years\",\"18 to 49 years\",\"50 to 64 years\"],[\"18 to 49 years\",\"65+ years\",\"18 to 49 years\",\"50 to 64 years\",\"18 to 49 years\",...,\"18 to 49 years\",\"0 - 17 years\",\"0 - 17 years\",\"18 to 49 years\",\"18 to 49 years\"],...,[\"50 to 64 years\",\"50 to 64 years\",\"50 to 64 years\",\"50 to 64 years\",\"50 to 64 years\",...,\"18 to 49 years\",\"18 to 49 years\",\"18 to 49 years\",\"18 to 49 years\",\"18 to 49 years\"],[\"18 to 49 years\",\"18 to 49 years\",\"18 to 49 years\",\"18 to 49 years\",\"18 to 49 years\",...,\"50 to 64 years\",\"50 to 64 years\",\"50 to 64 years\",\"50 to 64 years\",\"50 to 64 years\"]]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "table.num_rows"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "199550"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you're familiar with **pandas** and/or **SQL** it may come to you as a relieve that **Arrow Table** provides similar methods and APIs. <br>\n",
    "When in doubt, you can always convert Arrow Tables into pandas dataframes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "table.take([0, 1, 2]).to_pandas() # Get row 0, 1, and 2 and conver to pandas df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_month</th>\n",
       "      <th>res_state</th>\n",
       "      <th>state_fips_code</th>\n",
       "      <th>res_county</th>\n",
       "      <th>county_fips_code</th>\n",
       "      <th>age_group</th>\n",
       "      <th>sex</th>\n",
       "      <th>race</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>case_positive_specimen_interval</th>\n",
       "      <th>case_onset_interval</th>\n",
       "      <th>process</th>\n",
       "      <th>exposure_yn</th>\n",
       "      <th>current_status</th>\n",
       "      <th>symptom_status</th>\n",
       "      <th>hosp_yn</th>\n",
       "      <th>icu_yn</th>\n",
       "      <th>death_yn</th>\n",
       "      <th>underlying_conditions_yn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02</td>\n",
       "      <td>HI</td>\n",
       "      <td>15</td>\n",
       "      <td>HONOLULU</td>\n",
       "      <td>15003.0</td>\n",
       "      <td>18 to 49 years</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01</td>\n",
       "      <td>HI</td>\n",
       "      <td>15</td>\n",
       "      <td>MAUI</td>\n",
       "      <td>15009.0</td>\n",
       "      <td>0 - 17 years</td>\n",
       "      <td>Female</td>\n",
       "      <td>Multiple/Other</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12</td>\n",
       "      <td>HI</td>\n",
       "      <td>15</td>\n",
       "      <td>NA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18 to 49 years</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>NA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  case_month res_state  state_fips_code res_county  county_fips_code  \\\n",
       "0    2022-02        HI               15   HONOLULU           15003.0   \n",
       "1    2022-01        HI               15       MAUI           15009.0   \n",
       "2    2021-12        HI               15         NA               NaN   \n",
       "\n",
       "        age_group     sex            race        ethnicity  \\\n",
       "0  18 to 49 years  Female           Asian  Hispanic/Latino   \n",
       "1    0 - 17 years  Female  Multiple/Other  Hispanic/Latino   \n",
       "2  18 to 49 years  Female           Asian               NA   \n",
       "\n",
       "   case_positive_specimen_interval  case_onset_interval  process exposure_yn  \\\n",
       "0                              NaN                  NaN  Missing     Missing   \n",
       "1                              NaN                  NaN  Missing     Missing   \n",
       "2                              NaN                  NaN  Missing     Missing   \n",
       "\n",
       "              current_status symptom_status  hosp_yn   icu_yn death_yn  \\\n",
       "0  Laboratory-confirmed case        Missing  Missing  Missing  Missing   \n",
       "1  Laboratory-confirmed case        Missing  Missing  Missing  Missing   \n",
       "2  Laboratory-confirmed case        Missing  Missing  Missing  Missing   \n",
       "\n",
       "  underlying_conditions_yn  \n",
       "0                           \n",
       "1                           \n",
       "2                           "
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **So why all the trouble with Arrow?**\n",
    "On one hand Arrow stores the data in **columnar** format which makes a lot of computational routines substantially faster,<br>\n",
    "especially when iterating through large chunks of data. That is, because a columns are stored in contiguous memory, <br>\n",
    "which is exceedingly easier to read in chunks compared to row-based memory layouts, as illustrated here [4]: <br>\n",
    "\n",
    "![image](https://arrow.apache.org/img/simd.png)\n",
    "\n",
    "Secondly, it provides a language-agnostic way to pass tabular data from one application to another. <br>\n",
    "Last but not least, it also allows for a technique called zero-copy reads. <br>\n",
    "Instead of copying the data and passing it on, we can simply point to the location in memory without ever reading it [5]. <br>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So far, we have uploaded and modified our dataset only locally. <br> \n",
    "Let's see how we can make them accessible to other clients."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Starting the server\n",
    "Remote Arrow uses one server to manage datasets and exposes a fixed set of commands (upload, download, query) to all clients in the network. <br>\n",
    "This set of commands can, however, be extended and adapted depending on your specific needs. <br>\n",
    "\n",
    "As mentioned before, Remote Arrow uses the **Apache Arrow Flight API** which specifies a list of services that need to be supported (e.g. **doGet**, **doPut**, **doAction**). <br>\n",
    "This is reminiscent of the **REST** architecture style (**PUT**, **POST**, **DELETE**, **GET**). <br> \n",
    "The bottomline is that both simply define a uniform interface. We are, however, free in implementing our own custom **actions and features** and the way our data is being handled internally.\n",
    "\n",
    "In contrast to **REST** which uses **HTTP** for communication, **Arrow Flight** is built on top of gRPC. <br>\n",
    "Just like **Arrow** itself, **Arrow Flight** is also language-agnostic, <br>\n",
    "so you can develop clients in any language as long as it follows/supports the **Arrow Flight API** [4]. <br>\n",
    "\n",
    "In this tutorial, we are going to use Python to demonstrate client and server interactions. <br>\n",
    "First, let's start the server."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "SERVER_IP = \"localhost\"\n",
    "SERVER_PORT = \"5005\"\n",
    "\n",
    "server_process = subprocess.Popen([\"python3\",\"server.py\",\"--host\", SERVER_IP,\"--port\", SERVER_PORT])\n",
    "pid = server_process.pid\n",
    "time.sleep(2) # Wait for server to boot before running other cells"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The server is now running in the background as long as the notebook is active, i.e. its kernel is running. <br>\n",
    "Of course, you can also run it in a regular terminal if you want to read the console output, for example:\n",
    "\n",
    "`python3 server --host <HOST> --port <PORT>`\n",
    "\n",
    "For now, let's connect a client to the server and see how we can upload our local CSV file:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from RemoteArrow import RemoteDataset\n",
    "client_A = RemoteDataset(\"hawaii_covid.csv\", hostname=SERVER_IP+\":\"+SERVER_PORT)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we have instantiated a **RemoteDataset** object with a path to the local CSV file and the server's address. <br>\n",
    "Under the hood, our object (**client_A**) converted that file into an **Arrow Table** (just as above) and sent it to the server via **doPut**.\n",
    "\n",
    "It does so without having to serialize or convert the data again. The server simply receives it in batches and reconstruct the **Arrow Table** from it.\n",
    "\n",
    "The dataset now resides on the server and is available to all clients as a **Flight** with **ID 0**. <br>\n",
    "\n",
    "You will also find that the server has stored our file in the ***/datasets*** subdirectory in **.parquet** format. <br>\n",
    "The **.parquet** format allows for highly memory-efficient compression, when writing or reading Arrow Tables. <br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client_A.list_flights()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A **Flight** is nothing but an access point that corresponds to one dataset or data chunk. <br>\n",
    "A client may ask the server for all available Flights that it can access, i.e. download or query against. <br>\n",
    "\n",
    "Technically, a data chunk is also a \"dataset\" but for the sake of this tutorial we refer to original data (e.g. a CSV file) as **datasets** <br>\n",
    "and any temporarily generated data as **data chunks**, i.e. the result of a query. <br>\n",
    "In any event, both are internally handled as **Arrow Tables** and exposed as Flights.<br>\n",
    "\n",
    "Furthermore, instantiating **client_A** as a **RemoteDataset** also overrides the object's **Arrow Table methods** with corresponding RPC calls to our server. <br>\n",
    "What that means is that calling methods like **take** or **select**, as we did in the previous section above, <br>\n",
    "will not locally compute the query, but instead command the server to do the exact same query for us (on the dataset we just uploaded), <br>\n",
    "and return the result to us through the RPC channel. As mentioned before, it will also make the result available as a **Flight**.<br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "query_result = client_A.take([0,1,2]) # Will be executed on the server, and the result saved in our local variable \"query_result\"\n",
    "query_result.to_pandas() "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Query results are made available as new Flights\n",
    "client_A.list_flights()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Until now, it may not be totally clear why this is useful, if we could just have computed the same query locally <br>\n",
    "without having to send it to the server first. But keep in mind that **client_A** may be much more restricted <br>\n",
    "in computational power or memory resources in comparison to our server. <br>\n",
    "\n",
    "Also we may now delete the dataset on **client_A** (both on disk and in runtime memory). <br>\n",
    "\n",
    "\n",
    "Now, let's pretend to be another client (**client_B**) and see how we can interact with one of the Flights. <br>\n",
    "We can **inspect()** a Flight first, to get insight on its metadata (e.g. *schema*, *num of rows*, ...)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client_B = RemoteDataset() # Instantiate client with default host (localhost:5005) and no file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client_B.inspect(0) # Inspect Flight 0\n",
    "client_B.connect(0) # Connect to Flight 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Connecting to a Flight means that all of our local calls are once again overriden with **RPC calls** <br>\n",
    "that command the server to do the queries against the dataset associated with that Flight."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client_B.take([0,1,2,3,4,5]).to_pandas() # Call take() on Flight 0 and convert result to pandas"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that, **client_B** can now query against the original dataset (**hawaii_covid**) without ever downloading it. <br>\n",
    "\n",
    "This becomes incredibly efficient, when you want to query against a dataset that is extremly large, <br>\n",
    "or you simply don't want to download or store it in it's entirety. <br>\n",
    "\n",
    "One key aspect to understand, however, is that **only the first method call** that follows the RemoteDataset object (**client_B**), <br>\n",
    "invokes a remote procedure call. In the case above that means, only **client_B.take([0,1,2,3,4,5])** is being sent and processed by the server.<br>\n",
    "The result is being returned as an Arrow Table object and **locally** converted to a pandas dataframe. <br>\n",
    "\n",
    "\n",
    "If we want to download the entire dataset (or data chunk) of the Flight that our client is connected to, we can use **fetch()**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds = client_B.fetch() # Download the entire dataset of the connected Flight and store the result in \"ds\"\n",
    "\n",
    "# Convert it to pandas dataframe and continue to process it locally\n",
    "df = ds.to_pandas()\n",
    "print(\"Original num of rows: \", len(df))\n",
    "\n",
    "dropped = df.dropna()\n",
    "print(\"Num of rows without nulls: \", len(dropped))\n",
    "\n",
    "dropped[\"case_month\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, there is a lot of flexibility in switching between accessing local and shared datasets.<br>\n",
    "Making intermediate query results publicly available also creates an increased synergy between clients, <br>\n",
    "if they are working in the context of a larger application or system. <br>\n",
    "\n",
    "One client may upload a huge dataset, another can compute a bunch of queries against it and the third client has access to all of the results <br>\n",
    "and can cherry pick only the ones relevant to its task. In any case, we can eliminate redundancy and potentially boost performance and efficacy on all ends.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Actions\n",
    "\n",
    "**Apache Arrow Flight** defines another interface to interact with Flights, so-called **Actions**. <br>\n",
    "Actions are application-specific commands. Our server provides Actions for **saving, deleting, and clearing Flights** as well as **shutting down the server**. <br>\n",
    "\n",
    "Any client can send actions by properly setting the action **type** and **body** (payload) that are defined by our server. <br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client_C = RemoteDataset()\n",
    "client_C.list_actions()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client_C.action(type=\"save\", body=\"1 my_new_dataset\") # Request server to save Flight 1 as \"my_new_dataset.parquet\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client_C.action(\"clear\") # Clear all flights (not affecting .parquet files)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client_C.list_flights()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "All Flights have now been cleared and the list of available Flights is empty. <br>\n",
    "However, the files that have been saved should still be stored on our server. <br>\n",
    "\n",
    "Let's see what happens if our server crashes or reboots gracefully."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client_C.action(\"shutdown\")\n",
    "time.sleep(5) # Wait for server to properly shutdown before running other cells"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Restart the server\n",
    "server_process = subprocess.Popen([\"python3\",\"server.py\",\"--host\", SERVER_IP,\"--port\", SERVER_PORT])\n",
    "pid = server_process.pid\n",
    "\n",
    "time.sleep(2) # Wait for server to boot\n",
    "client_D = RemoteDataset()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should see that the server recovered the saved datasets. <br>\n",
    "As a final step in this tutorial, we can delete them via the \"delete\" actions. The corresponding .parquet files will also be wiped from the server."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client_D.action(\"delete\", \"0\")\n",
    "client_D.action(\"delete\", \"1\")\n",
    "\n",
    "client_D.list_flights()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that, the IDs did not disappear and were not rearranged, inorder to maintain consistency throughout the runtime of the server. <br>\n",
    "This is necessary when deleting specific Flights one by one, whereas \"clearing\" all Flights basically removes the necessity to maintain that order. <br>\n",
    "\n",
    "Of course you can implement many more actions, such as *deleting all files*, *renaming a file*, *rebooting the server*, *check server health*, etc., <br>\n",
    "the list is virtually endless, which underlines the versatility and extensability of **Arrow** and **Arrow Flight**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "In this tutorial, we have examined how **Remote Arrow** can help to outsource the burden of computation, storage and shared access of data on a single server. <br>\n",
    "Though still in its prototype phase, it can offer tremendous flexibility for managing shared tabular data.<br>\n",
    "This effect becomes more evident when datasets are used in a network of heterogenous applications that work in different languages, <br>\n",
    "even more so when the shared data is extremely large and/or the clients low in computational prowess.\n",
    "\n",
    "Another aspect, we have not discussed in-depth yet is the remarkable boost in transmission speed, that has been benchmarked by *Ahmad et al.* [4]. <br>\n",
    "Their publication cites that *\"Flight is able to achieve up to **6000 MB/s and 4800 MB/s throughput**\"* <br>\n",
    "and companies like **Dremio** that use Arrow Flight **perform 20x and 30x better** compared to traditional connection methods. <br>\n",
    "That means Arrow Flight becomes an even better choice for applications that require long-haul communication over the internet. <br>\n",
    "\n",
    "### Limitations\n",
    "One major limitation is that the Remote Arrow server is a single-point of failure. <br>\n",
    "It is also synchronous and can only serve on request at a time, making it <br>\n",
    "suboptimal for high-frequency applications or systems that require massive scaling. <br>\n",
    "Some of these problems may be ameliorated by instantiating multiple servers and setting up a load-balancer between clients and servers. <br>\n",
    "This, however, comes at the cost of increased complexity for managing the shared data and handling asynchronous access and race conditions. <br>\n",
    "\n",
    "**Arrow Flight** is also still in its developing stage and thus not as stable or prevalent as more established frameworks for data transfers. <br>\n",
    "Moreover, the API and documentation is rather opaque and can be somewhat restrictive, where a lot of the development of **Remote Arrow** consisted of <br>\n",
    "workarounds and trial & error. However, we are confident that **Arrow** and **Arrow Flight** will find their way in becoming industry-standards as time progresses.\n",
    "\n",
    "\n",
    "### Future work\n",
    "If you're interested in contributing or developing your own application, please refer to the sources above and check out the Git repositories. <br>\n",
    "**Remote Arrow** could be extended by developing new actions, scrutinizing and benchmarking current features, as well as supporting different file formats. <br>\n",
    "Writing **Remote Arrow** clients in different languages would also be helpful, especially when testing it in an in-vivo multi-client system. <br>\n",
    "\n",
    "Regardless of the use-case, hopefully you have learned that **Arrow** especially in combination with **Arrow Flight** provides a highly performant framework for working with big data."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.12 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}